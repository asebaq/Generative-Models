Pytorch Implementation of vanilla DDPM with einops
 
* Used 2D Linear Attention instead of vanilla self-attention for reasonable memory usage

* Used CelebA-HQ 256x256 resized dataset : https://www.kaggle.com/badasstechie/celebahq-resized-256x256

#### Result from Trained model
Result for MNIST(32x32), CIFAR10(32x32), CelebA-HQ(128x128) respectively

<img src="https://user-images.githubusercontent.com/48702949/139891786-922270ea-a833-4760-8374-50e2599b4d34.jpg" width="240" height="240"/> <img src="https://user-images.githubusercontent.com/48702949/139892619-3f7986f9-202b-4a04-9ccd-20a379df9dbc.jpg" width="240" height="240"/> <img src="https://user-images.githubusercontent.com/48702949/139892655-55423eab-3304-41df-b680-b60958e0090a.jpg" width="240" height="240"/>
